---
layout:     post
title:      看不懂我吃屎系列——BERT
subtitle:   
date:       2019-07-04
author:     高庆东
header-img: img/ceshi.jpg
catalog: true
tags:
    - Transformer
    - 时许数据
    - BERT
    - 空间映射
    - python
    - pytorch
---

### 网络结构
先吹一波，这个网络牛逼就牛逼再虽然移除了循环神经网络但依然可以处理时序数据，牛逼。根本在于  
注意力机制，这个机制确实是玄学，就是让每个输入的词都和其他词有一个关系，然后训练这个关系  
牛逼这这了，这个关系也很好理解，就是所有词计算一个概率向量，然后原始的数据乘这个向量。 
说白了这个网络结构就是个全连接网络的升级，本质上依然是全连接。简化点说，一个数据输入到全  
连接网络中，然后获得一个向量，这个向量再做下一步操作，分类了，解码了等等。  
这个网络主要有两部分构成，第一是映射编码，第二是Transformer

### 映射编码
这个部分就是把索引的数据映射成向量，   
先说一下输入的数据，以NLP为例，现在有个词典，这个词典包含了一些词，假如有一千个词， 
有一条数据这条数据是一句话，这句话可以用词表示出来，这句话中的每个词都可以在词典中  
找到，重点来了，这句话可以表示成这句话中每个词在词典中的位置，  
比如词典是这个{'我'：0，'你'：1，'是'：2，'傻':3,'逼':4}，有句话“你是傻逼” 这句话  
可以表示成一个列表`[1，2，3，4]` 这就是我们的输入数据，需要待映射的数据， 
```
class BERTEmbedding(nn.Module):

    def __init__(self, vocab_size, embed_size, dropout=0.1):
           #vocab_size 表示词典大小，embed_size表示我们想要的映射空间，
        super().__init__()
        self.token = nn.Embedding(vocab_size=vocab_size, embed_size=embed_size) #普通embedding 
        self.position = PositionalEmbedding(d_model=self.token.embedding_dim)
        self.segment = SegmentEmbedding(embed_size=self.token.embedding_dim)
        self.dropout = nn.Dropout(p=dropout)
        self.embed_size = embed_size

    def forward(self, sequence, segment_label):
        x = self.token(sequence) + self.position(sequence) + self.segment(segment_label)
        return self.dropout(x)

```

