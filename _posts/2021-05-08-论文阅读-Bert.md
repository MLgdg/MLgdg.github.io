---
layout:     post
title:      论文阅读-BERT
subtitle:   
date:       2021-05-07
author:     高庆东
header-img: img/ceshi.jpg
catalog: true
tags:
    - BERT
    - 文本分类
---

## 模型
区别与GPT单向的局限性，有些任务需要通读全文才能理解  
GPT不能通读全文。具体的操作就是去掉MASK部分  
提出了两个任务  
1、MLM 任务 掩码语言模型简单说就是一句话中遮住一些词或者置空或者用其他文本替换  
然后目标是对其还原  
2、匹配任务，两句文本是否是前后句的关系，使用cls训练  
模型结构炒鸡简单和GPT一样只不过mask矩阵是paddmask+遮挡位置的mask  
训练任务略有不同
