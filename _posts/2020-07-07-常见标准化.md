---
layout:     post
title:      常见标准化
subtitle:   
date:       2020-07-07
author:     高庆东
header-img: img/ceshi.jpg
catalog: true
tags:
    - python
    - 标准化
---

## 标准化算法

#### Batch Normalization (BN) 
是最早出现的，也通常是效果最好的归一化方式。N×C×H×W包含 N 个样本，  
每个样本通道数为 C，高为 H，宽为 W。对其求均值和方差时，将在 N、H、W上操作，  
而保留通道 C 的维度。具体来说，就是把第1个样本的第1个通道，加上第2个样本第1个通道  
… 加上第 N 个样本第1个通道，求平均，得到通道 1 的均值（注意是除以 N×H×W   
而不是单纯除以 N，最后得到的是一个代表这个 batch 第1个通道平均值的数字，  
而不是一个 H×W 的矩阵）。求通道 1 的方差也是同理。对所有通道都施加一遍这个操作，  
就得到了所有通道的均值和方差。
#### Layer Normalization
就是N个样本 每个样本求均值，相当于一张图片内rgb通道上所有像素和除（C\*W\*H）
#### Instance Normalization
这个更简单，再通道级别上做标准化，拿出一个通道，通道上像素求和再求均值就是除H*W

IN适用于生成模型中，比如图片风格迁移。因为图片生成的结果主要依赖于某个图像实例，  所以对整个Batch进行Normalization操作并不适合图像风格化的任务，在风格迁移中适  
用IN不仅可以加速模型收敛，并且可以保持每个图像实例之间的独立性。

与此同时，BN适用于判别模型，比如图像分类模型，因为BN注重对每个Batch进行Normalization操作，  
从而保证数据分布的一致性，而判别模型的结果正是取决于数据整体分布。但是BN对BatchSize的大小  比较敏感，由于每次计算均值和方差是在一个Batch上，所以如果BatchSize太小，则计算的均值和方  
差不足以代表整个数据分布。
