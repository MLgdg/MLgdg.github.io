---
layout:     post
title:      看不懂我吃屎系列——CTPN
subtitle:   
date:       2019-05-30
author:     高庆东
header-img: img/ceshi.jpg
catalog: true
tags:
    - CTPN
    - LSTM
    - RPN
    - 文字识别
    - 非极大值抑制
    - ROIpooling
    - VGG16
    - python

---

### CTPN
主要是作用是来检测图片上的文字对象，并画一个框出来，目的是画出可能是文字的框，下一步的输入可以是ocr识别，先说一下他的网络结果，  
其很就是在FasterRCNN的基础上增加一个RNN的结构，看了一下之前的FasterRCNN好像有些没说明白，这次全部写了。从结构到代码。结合代  
码说原理。它的大结果快主要有VGG，RNN，RPN 分类回归，非极大值抑制用来处理最终预测结果  

### 先介绍一个我们需要的训练数据和处理结果
输出当然是一张图片和图片的标签，也就是图像的框坐标数据，在CTPN中没有类别数据因为判断是不是文字直接就是二分类问题。 
先说图片，首先读取一张图片然后读标签坐标然后生成锚点框坐标，为啥进行这个操作呢，因为我们经过VGG后生成的特征图与原始图像有一个比  
例关系，通过这个比例来计算锚点个数和框坐标。具体操作看代码吧
```
#使用torch写代码所以你懂的，写一个处理数的子类，该类可以生成一个样本的训练数据
class VOCDataset(Dataset):
    def __init__(self,
                 datadir,
                 labelsdir):
        '''
        :param txtfile: image name list text file
        :param datadir: image's directory
        :param labelsdir: annotations' directory
        '''
        if not os.path.isdir(datadir):
            raise Exception('[ERROR] {} is not a directory'.format(datadir))
        if not os.path.isdir(labelsdir):
            raise Exception('[ERROR] {} is not a directory'.format(labelsdir))

        self.datadir = datadir
        self.img_names = os.listdir(self.datadir)
        self.labelsdir = labelsdir

    def __len__(self):
        return len(self.img_names)

    def __getitem__(self, idx):
        img_name = self.img_names[idx]
        img_path = os.path.join(self.datadir, img_name)
        print(img_path)
        xml_path = os.path.join(self.labelsdir, img_name.replace('.jpg', '.xml'))
        gtbox, _ = readxml(xml_path)
        img = cv2.imread(img_path)
        h, w, c = img.shape
        # clip image
        if np.random.randint(2) == 1:
            img = img[:, ::-1, :]
            newx1 = w - gtbox[:, 2] - 1
            newx2 = w - gtbox[:, 0] - 1
            gtbox[:, 0] = newx1
            gtbox[:, 2] = newx2

        [cls, regr], _ = cal_rpn((h, w), (int(h / 16), int(w / 16)), 16, gtbox)  #[labels, bbox_targets], base_anchor
        #cls表示经过iou和其他方式筛选的类别里面主要是-1，0，1  #regr表示所有锚点框回归的关系数
        m_img = img - IMAGE_MEAN

        regr = np.hstack([cls.reshape(cls.shape[0], 1), regr])

        cls = np.expand_dims(cls, axis=0)  #扩展维度

        # transform to torch tensor
        m_img = torch.from_numpy(m_img.transpose([2, 0, 1])).float()  
        cls = torch.from_numpy(cls).float()
        regr = torch.from_numpy(regr).float()

        return m_img, cls, regr   #data中的coll_fn哪个函数默认的时候会自动再外面加一维，为batchsize维

```
看看计算锚点的代码，这个代码的逻辑是，首先定义一个锚点对应10个不同比例的框，然后计算一共有多少锚点然后计算每个锚点框的在原图的坐标  
再原图上标出来  
```
def gen_anchor(featuresize, scale):
    """
        gen base anchor from feature map [HXW][9][4]
        reshape  [HXW][9][4] to [HXWX9][4]
    """
    #设定不同尺寸的锚点框每个锚点对应10个框
    heights = [11, 16, 23, 33, 48, 68, 97, 139, 198, 283]
    widths = [16, 16, 16, 16, 16, 16, 16, 16, 16, 16]

    # gen k=9 anchor size (h,w)
    heights = np.array(heights).reshape(len(heights), 1)
    widths = np.array(widths).reshape(len(widths), 1)
    # 计算一个锚点框的中心坐标和大小，
    base_anchor = np.array([0, 0, 15, 15])
    # center x,y
    xt = (base_anchor[0] + base_anchor[2]) * 0.5
    yt = (base_anchor[1] + base_anchor[3]) * 0.5
    #计算一个锚点对应的所有框的的坐标，
    # x1 y1 x2 y2
    x1 = xt - widths * 0.5
    y1 = yt - heights * 0.5
    x2 = xt + widths * 0.5
    y2 = yt + heights * 0.5
    base_anchor = np.hstack((x1, y1, x2, y2))  #水平方向堆叠
    #将锚点分布到原图上时锚点的坐标
    h, w = featuresize
    shift_x = np.arange(0, w) * scale
    shift_y = np.arange(0, h) * scale
    # apply shift
    anchor = []
    for i in shift_y:
        for j in shift_x:
            # 画出所有在原图上的锚点框的坐标
            #这里区分一下，锚点时一个缩放后中心点，这个中心点对应10个框就是锚点框。
            anchor.append(base_anchor + [j, i, j, i])
    return np.array(anchor).reshape((-1, 4))
```

计算每个锚点框针对实际边框的IOU值，加入一张图片上有3个框，那就拿出一个锚点框，分别计算每个的IOU，结果存储，遍历所有的锚点框  
然后会生成一个矩阵矩阵元素表示第几个锚点框与第几个实际框的IOU的值。

```
def cal_overlaps(boxes1, boxes2):
    """
    boxes1 [Nsample,x1,y1,x2,y2]  anchor
    boxes2 [Msample,x1,y1,x2,y2]  grouth-box

    """
    area1 = (boxes1[:, 0] - boxes1[:, 2]) * (boxes1[:, 1] - boxes1[:, 3])
    area2 = (boxes2[:, 0] - boxes2[:, 2]) * (boxes2[:, 1] - boxes2[:, 3])

    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))

    # calculate the intersection of  boxes1(anchor) and boxes2(GT box)
    for i in range(boxes1.shape[0]):   #遍历每个锚点框
        overlaps[i][:] = cal_iou(boxes1[i], area1[i], boxes2, area2)  #依次比较每个IOU
    return overlaps
```
通过某些过滤条件过滤掉一些框，过滤条件有IOU阈值等等，顺便为一些框打标签这个标签主要依据就是IOU阈值，大于某个阈值认为是
```
    anchor_argmax_overlaps = overlaps.argmax(axis=1)  # 每个锚点对应的GT的最大iou的位置
    anchor_max_overlaps = overlaps[range(overlaps.shape[0]), anchor_argmax_overlaps]  #每个锚点对应所有的GT的最大iou的位置

    # IOU > IOU_POSITIVE
    labels[anchor_max_overlaps > IOU_POSITIVE] = 1
    # IOU <IOU_NEGATIVE
    labels[anchor_max_overlaps < IOU_NEGATIVE] = 0
    # ensure that every GT box has at least one positive RPN region
    labels[gt_argmax_overlaps] = 1
```
