---
layout:     post
title:      温习常用算法（1）
subtitle:   
date:       2018-08-10
author:     高庆东
header-img: img/ceshi.jpg
catalog: true
tags:
    - HMM
    - 朴素贝叶斯
    - Adaoost
    - 随机森林
---

# 白话常用算法

## 决策树
有一堆数据数据有标签有特征，首先计算每类特征的熵值，根据相对熵

或者基尼系数再或者熵来做树节点的选择，再设定分叉的条件，最终每个

特征可能都会构成一层，直到所有特征选择完成，一棵决策树就生成了

贼鸡儿容易过拟合，基本没泛华能力。使用的时候就是，有条数据过来了

首先把数据的特定特征输入到树的根节点，决策判断输出一个结果，再把

数据的另一个特定特征输入到下一个节点，直到进行下去最后输出。


## 随机森林

森林意思有很多棵树，意思是有上面的决策树很多棵。巧妙的地方在于把

数据集随机分成M个子集 （有放回的采样）每个子集生成一棵决策树，好

了，随机森林完成了，接下来是使用，来了一条数据，这个数据分别输入

到M棵树中，每棵树会输出一个结果，根据问题选择取投票结果或者均值

结果等等，这样不会过拟合，基本没啥卵用。（个人浅见）

## 逻辑回归

二项分布下的极大似然估计这样说可能比较垃圾，说是回归其实这个是个

分类模型，模型的输出是类别概率，首先有个线性表达式，再把这个线性

表达式规范在0到1之间 sigmoid函数做规范，使用互熵损失，做训练。

有条数据过来了，把数据特征输入到函数，输出一个概率值，这个值是

属于这个类别的概率有多大。


## SVM
牛逼的算法不多解释有多牛逼反正就是很吊，在类别间画一条线，把两个

分开，这条线可以有无数形式，但最佳只有一个，这个就是离连个类别都

很远的那条线，这样就有函数了，找到最远距离的那条线，点到线的距离

高中就学过了，要使这个距离最大，且有个条件是，这条线可以正确的分

开样本，到这就完了，问题本质是拉格朗日条件极值，至于核函数其实就

是把样本维度太高，让在那个维度上线性可分。


##朴素贝叶斯
朴素连个字因为要求特征独立所以比较朴素， 这是个分类器，所有要明确

类别，一共有几类，分类器的输出是每个类别的概率，取概率最大的那个

作为最终的预测结果。这次要举个例子，假如要判断一句话是好还是坏，

首先有个语料库，语料库包含了所有可能的词，每个词对应在每个类别中

有个概率。每个类别都有个概率，现在有句话，要判断这句话是好是坏，先

把这句话的词分出来，要判断在这些词的条件下这句话是好是坏的概率，可

以根据贝叶斯公司转化后在语料库中的计算得到

计算方法是有一条数据 或者一句话，这句话中的词或者数据中的特征是独

立的，把特征词输入到模型中输出类别概率 

贝叶斯不能考虑考虑词间的关联性。

![朴素贝叶斯算法步骤](/img/朴素贝叶斯算法.png)

## K最近邻
有个数据现在要判断这个数据属于哪个类别这个问题是分类问题，首先需要

一个训练集，这个模型不需要训练，有一条数据分别计算这条数据与每个类

别的距离，这个距离是这条数据与某个类别中所有样本的距离均值，取均值

最小那个，那个就是可能类别，这个缺点是需要存所有训练数据。


## K均值
这是个无监督的学习算法，最重要的参数是你准备分几类，首先确定一种计

算距离的方法然后随机找K个点，分别计算样本距离这K个点的距离，离哪个

点近，就将样本分配给那个类别，将样本分类一遍，再计算分类后样本的均

值中心，再重新计算样本与这个中心的距离再分类，一直这样下去。











