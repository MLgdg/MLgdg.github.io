---
layout:     post
title:      论文阅读-BEITv2
subtitle:   
date:       2022-05-07
author:     高庆东
header-img: img/ceshi.jpg
catalog: true
tags:
    - 多模态
    - Image
---


## 模型流程
1、训练图像编码器

常见的Image token 有三种方式   
#### grid feature
这种就是取卷积后的特征图，每个点就是个token  
#### region feature
这个比较简单就是 目标检测的结果，框出来之后的特征作为token
#### patch feature
直接切图片然后提取特征
#### VQ-VAE

## 注意
视觉任务中一般用相对位置编码

## 论文提出的观点和使用的方法
1、blockwise masking
2、相对位置编码  
3、向量化知识蒸馏VQ-KD
4、EmbeddingEMA 滑动平均避免只有少部分Embedding有效
MAE对比  
MAE中掩码的比率非常高，达到 75%。相对的，在 BERT 中，  
对文本数据的掩码率为 15%。这体现出图像数据的冗余性和文本数据的高度语义性


## 训练细节
训练 tokenizer 时，由于中间的最近邻查表操作是不可微的，为了梯度反传，  
可将 decoder 输入的梯度直接拷贝到 encoder 输出。因为 quantizer   
查找的是每个编码器输出的最近邻 embedding，码本 embedding 的梯度可  
以为编码器指示合理的优化方向；为了稳定码本的训练并提高利用率，避免码本坍塌，  
导致只有一小部分 embedding 会被使用，tokenizer 的训练采用了一些 trick。  
其中包括使用标准化 l2 距离、降低 embedding 维度到 32 维、滑动指数平均 （EMA）；

