---
layout:     post
title:      深度学习
subtitle:   
date:       2017-12-06
author:     高庆东
header-img: img/ceshi.jpg
catalog: true
tags:
    - pca
    - 自编码网络
    - 编码
    - 深度学习
---

# 深度学习网络

对于图片描述网络的构建
Rnn网络的隐层与隐层之间是全连接的

![图片描述网络](/img/图片描述网络.png)


## 自编码器

输出的目标是和输入一样 让输出逼近输入的过程

![自编码网络](/img/自编码网路.png)

![自编码网络解释](/img/自编码网络解释.png)

![自编码网络算法](/img/自编码网络算法.png)

- 对于pca方法选择k的个数是看特征值的比重
- 对于一幅图像将所有像素值加起来求均值然后减去均值的意义是：图像的内容与图像平均亮度无关 做图像pca处理时使用该方法去均值

- 对于softmax分类和logistic分类 如果分出来的类是互斥的则用softmax不互斥用logistic

## 栈式自编码算法
一般用于解决小图像小图块的识别

由多层的自编码网络组成 每层网络单独训练以输入等于输出为目的完成训练后得到隐藏层的输出 （除输入和输出层外以隐藏层的输出作为输入） 
栈式自编码网络每层都具有学习特征的能量层数的加深学习到的特征会越来越深刻

![线性自编码网络](/img/线性自编码网络1.png)

![线性自编码网络](/img/线性自编码网络2.png)

![线性自编码网络](/img/线性自编码网络3.png)

## 线性解码器
对于上述的自编码器输出层的激励函数时sigmoid函数或者tanh函数 在训

练时需要将输入也同样放在（0,1）或者（-1,1）范围内 但是有些数据缩

放起来并不会有好的效果，所以采用线性解码器：输出层的激活函数为f(x)

=x。这样可以实现用实值输入训练网络

隐含层任然用sigmoid，或者tanh函数做激活函数

