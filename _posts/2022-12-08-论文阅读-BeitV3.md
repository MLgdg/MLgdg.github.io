---
layout:     post
title:      论文阅读-BEITv3
subtitle:   
date:       2022-12-07
author:     高庆东
header-img: img/ceshi.jpg
catalog: true
tags:
    - 多模态
    - Transformer
---

## 多模态大模型
延用了Beitv2的训练方式不通点在于  
1、多模态
同时输入文本图像的图文对
建设一个专家系统，每个模态对应了一个专家系统  
起始就是每个模态对应一下全连接，训练方式除了v2版本的还有一些模态匹配的  
任务，综合起来就是mask data modeling 还原被mask的部分
![beitv3](/img/20230313/beitv3.jpg)
## 预训练
论文证明mask data modeling 才是预训练任务最正确的方式  
对比学习预训练对batchsize比较敏感，batch越大效果越好，但是  
batch大对资源是一种考验

