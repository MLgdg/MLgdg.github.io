---
layout:     post
title:      大模型训练
subtitle:   
date:       2023-04-15
author:     高庆东
header-img: img/ceshi.jpg
catalog: true
tags:
    - FSDP
    - 大模型训练
---


## Pytorch
使用常规1.12以后的pytoch中的FSDP进行训练  
### 函数解析 
#### DistributedSampler 
对epoch内数据进行拆分，每个epoch内数据按照卡数拆分，有随机和顺序两种方式  
DistributedSampler返回的是数据位置索引，这里的rand表示当前进程中的第几张卡  
total_size表示一个epoch内有多少条数据 num_replicas=total_size/一共的卡数  
最终返回的是一个数据位置索引迭代器。  

"""
    def __iter__(self) -> Iterator[int]:
        if self.shuffle:
            # deterministically shuffle based on epoch and seed
            g = torch.Generator()
            g.manual_seed(self.seed + self.epoch)
            indices = torch.randperm(len(self.dataset), generator=g).tolist()  # type: ignore[arg-type]
        else:
            indices = list(range(len(self.dataset)))  # type: ignore[arg-type]
        if not self.drop_last:
            # add extra samples to make it evenly divisible
            padding_size = self.total_size - len(indices)
            if padding_size <= len(indices):
                indices += indices[:padding_size]
            else:
                indices += (indices * math.ceil(padding_size / len(indices)))[:padding_size]
        else:
            # remove tail of data to make it evenly divisible.
            indices = indices[:self.total_size]
        assert len(indices) == self.total_size
        # subsample
        indices = indices[self.rank:self.total_size:self.num_replicas]
        assert len(indices) == self.num_samples
        return iter(indices)
"""

#### spawn
spawn函数包括了模型训练的整个周期，包括数据读取，模型训练更新和保存参数
mp.spawn(fn, nprocs,args)   
该函数的作用是生成使用 args参数运行fn的nprocs个进程。  
fn:第一个参数是一个函数，每个进程要运行的函数。  
nprocs: 第二个参数是开启的进程数目。  
args:第三个参数是fn的函数实参,需要注意的是fn函数的第一个参数是进程的id,不用写入args中，spawn会自动分配。  

如果我们有三张卡，那会启动三个进程 每个进程内都是单独读取数据，数据读取采样器使用DistributedSampler  
而该函数的输入包括rank也就是gpu号，和随机种子，而随机种子由epoch控制，所以根据读取方式每张卡都可以读到epoch内  
不重复的数据    
