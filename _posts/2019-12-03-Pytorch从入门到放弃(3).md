---
layout:     post
title:      深度学习Pytorch从入门到放弃(3)
subtitle:   
date:       2019-12-03
author:     高庆东
header-img: img/ceshi.jpg
catalog: true
tags:
    - Pytorch
    - 深度学习
    - 神经网络
    - python
    - 源码解析
    - c++底层
    - torch深度学习
    - nn模块

---

## NN模块的详细
### functional.py
该包主要是模型中计算方法的函数，过程，包括卷积。pooling 损失函数等，需要相比于类方法需要设定好输入参数，比如卷积核的实际大小。  
其中有个自适应的pooling非常好用，设定好输出大小可以自适应的将数据规整为目标大小。 
平时引用的时候，可以实现任意大小的图像进行计算。还有一些损失的计算，当损失函数中的某些参数不用作为超参数时，可以使用  
其中的方法计算损失。需要知道的是任何参数形式都存在两个很重要的属性。`data`和`grad`,这是构成节点和训练的关键。               

```
import torch.nn.functional as F
out F.adaptive_avg_pool2d()

```
这个模块就是各种方法的基础，在这个方法中有些和`torch.functional` 一样有些不一样。将来的趋势是后者代替前者。但是目前有些任然是。
相互不包含的。在类中一般不用这么直接的函数来实现。通常使用它们的类方法。可以方便操作参数，而且初始化的的时候也可以方便初始化权  
值和偏置。说到初始化就不得不提`init.py`,看清楚了这可是`init.py`不是`__init__.py`

### init.py
该文件主要用来初始化数据和参数，简单梯度操作，函数增益计算和参数赋值，这个文件全是函数，不是类对象，计算的时候需要首先传输参数  
在这些函数中都是inplaec操作，简单来说就是不会生产备份文件，只有一个文件，一个改变另一个也改变就是inplace操作
主要函数有  ：

```
import torch 
import torch.nn as nn

w=torch.empty(3,4)
Inint=nn.init.uniform_(w,a,b)#均匀分布,a,b范围
Inint=nn.normal(w,a,b)#高斯分布，a均值，b放差
init=nn.ones_(w)
```
初始化的方法有很多，核心还是均匀分布或者高斯分布，只不过条件不一样，加以一些限制性的条件，其实大同小异。对于训练模型来说不论哪  
种方法都不能证明是最好的，

### data_parallel.py
主要用来多核训练，将模型转化为多核模型，一定要注意数据，数据也需要是相应的形式，才能计算结果，代码上说数据在任何设备都行，但是  
在实际使用的时候不行，这个代码没看，暂时不看，反正是实现多gpu计算的说实话没有快多少。  
具体的工作原理是，先确定一个主GPU在，一般默认为0，首先将模型方到GPU0中，在进行DataParallel，之后将数据放入GPU0中，进行计算，  
多GPU下，是将模型复制多份，没份都放到各个GPU中，将batch数据均匀拆分，分给各个GPU，计算结果汇集结果，求loss，求导，再将导数  
分给各个GPU更新

 ```
import torch
import torch.utils.data as data
import torch.nn as nn

 class mode_1(nn.Module):  #构建模型
    def __init__(self):
        super(mode_1,self).__init__()
        self.l1=nn.Linear(10,10)
        #self.l2=nn.Linear(10,10)
        self.l3=nn.Linear(10,1)
    def forward(self,x):
        return self.l3(self.l1(x))
class data_(data.Dataset):  #构建数据
    def __init__(self):
        self.a=torch.rand(20,10)
    def __getitem__(self,ind):
        return self.a[ind]
    def __len__(self):
        return len(self.a)
a=data_()
test_a=torch.utils.data.DataLoader(a,batch_size=4)

mode=nn.DataParallel(mode_1(),[0,1])   #多GPU
mode.cuda(0) # 
for i in test_a:
    input = i.cuda(0) #将数据放到GPU中
    output = mode(input)
    print(output)

 #常用的一些判别方法
print(torch.cuda.is_available())
print(torch.cuda.get_device_name())
print(torch.cuda.device_count())
device = torch.device('cuda:0')

 ```
 
 
 
 ### modules
 这个模块是最核心的，主要用来构建模型，计算，优化等，其中最重要的module.py文件是最重要的。里面包和了各中基础方法，后面我会整理  
 个方法说明和使用手册。其中的`Module()`类是所有模块的基础，当我们需要构建一个模块时，需要继承这个类，同时需要继承该模块的初始  
 的方法。非常直接的方法。可以将模型导入到GPU当中。gpu的计算也很简单，先说说GPU的计算原理。  
注意在使用GPU时，优化前一点要后创建
 
 > gpu里有计算核的概念，多个线程可以调用同一个计算核，而且对线程调度也有一套东西，简单来说首先将数据转化为多个线程，各个线程进入
 一cuda核，每个cuda核作为一个线程。所以cuda核越多计算越快，我1060的显卡最多是1280cuda核的，反正不快，在计算过程中先将任务  
 差分成很多小任务具体咱也不知道咋拆，大概就是分成很多小进程然后再合并。其中主要通过线程号来控制线程，具体可以看这个 [GPU计算](https://www.cnblogs.com/skyfsm/p/9673960.html)  

使用`Module()`构建一个模块

```
imoport torch
r'''
构建一个全连接的网络模型 
'''
class NN(torch.nn.Module):
    def __init__(self,in_size,out_size):
        super(NN,self).__init__()#继承父类中的缓存器，参数NN时当前的类名
        self.L1=nn.Linear(in_size,100)
        self.L2=nn.Linear(100,100)
        self.L3=nn.Linear(100,100)
        self.L4=nn.Linear(100,out_size)
    def forward(self,x):
        out=self.L1(x)
        out=self.L2(out)
        out=self.L3(out)
        out=self.L4(out)
        return out
mode=NN(10,1)
```

这个NN模块会继承所有父类方法。  
次要关键的就是依据`Module()`类构建的基础模块，包括，卷积，LSTM，线性计算，损失函数(损失函数中有些有超参数，可以进行求导计算)   
还有标准化中的参数，Dropout，pooling。这些模块其中有些我下周之前会分析完。算了直接开始分析吧。
####  conv.py


 
