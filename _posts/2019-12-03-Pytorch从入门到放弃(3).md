---
layout:     post
title:      深度学习Pytorch从入门到放弃(3)
subtitle:   
date:       2019-12-03
author:     高庆东
header-img: img/ceshi.jpg
catalog: true
tags:
    - Pytorch
    - 深度学习
    - 神经网络
    - python
    - 源码解析
    - c++底层
    - torch深度学习
    - nn模块

---

## NN模块的详细
### functional.py
该包主要是模型中计算方法的函数，过程，包括卷积。pooling 损失函数等，需要相比于类方法需要设定好输入参数，比如卷积核的实际大小。  
其中有个自适应的pooling非常好用，设定好输出大小可以自适应的将数据规整为目标大小。 
平时引用的时候，可以实现任意大小的图像进行计算。还有一些损失的计算，当损失函数中的某些参数不用作为超参数时，可以使用  
其中的方法计算损失。需要知道的是任何参数形式都存在两个很重要的属性。`data`和`grad`,这是构成节点和训练的关键。               

```
import torch.nn.functional as F
out F.adaptive_avg_pool2d()

```
这个模块就是各种方法的基础，在这个方法中有些和`torch.functional` 一样有些不一样。将来的趋势是后者代替前者。但是目前有些任然是。
相互不包含的。在类中一般不用这么直接的函数来实现。通常使用它们的类方法。可以方便操作参数，而且初始化的的时候也可以方便初始化权  
值和偏置。说到初始化就不得不提`init.py`,看清楚了这可是`init.py`不是`__init__.py`

### init.py
该文件主要用来初始化数据和参数，简单梯度操作，函数增益计算和参数赋值，这个文件全是函数，不是类对象，计算的时候需要首先传输参数  
在这些函数中都是inplaec操作，简单来说就是不会生产备份文件，只有一个文件，一个改变另一个也改变就是inplace操作
主要函数有  ：

```
import torch 
import torch.nn as nn

w=torch.empty(3,4)
Inint=nn.init.uniform_(w,a,b)#均匀分布,a,b范围
Inint=nn.normal(w,a,b)#高斯分布，a均值，b放差
init=nn.ones_(w)
```
初始化的方法有很多，核心还是均匀分布或者高斯分布，只不过条件不一样，加以一些限制性的条件，其实大同小异。对于训练模型来说不论哪  
种方法都不能证明是最好的，

### data_parallel.py
主要用来多核训练，将模型转化为多核模型，一定要注意数据，数据也需要是相应的形式，才能计算结果，代码上说数据在任何设备都行，但是  
在实际使用的时候不行，这个代码没看，暂时不看，反正是实现多gpu计算的说实话没有快多少。

 ```
 net=torch.nn.DataParallel(model,device_ids=[0,1,2])
 out=net(input_data)
 ```
 
